{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuuC46ugZhGwe6vMMelly7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/conjt1111/UCS420/blob/main/A_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment** 10"
      ],
      "metadata": {
        "id": "flwAQqH-PJ2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n",
        "technology, food, books, etc.).\n",
        "1. Convert text to lowercase and remove punctuaƟon using re.\n",
        "2. Tokenize the text into words and sentences.\n",
        "3. Split using split() and word_tokenize() and compare how Python split and NLTK’s\n",
        "word_tokenize() differ.\n",
        "4. Remove stopwords (using NLTK's stopwords list).\n",
        "5. Display word frequency distribuƟon (excluding stopwords)."
      ],
      "metadata": {
        "id": "ZbKgyE37Mx62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "paragraph = \"\"\"Books open the door to countless worlds, ideas, and perspectives. Reading provides an escape from reality and offers a chance to learn something new every day. Whether it’s fiction or nonfiction, books allow deep connections with characters and concepts. They stimulate the imagination and enhance vocabulary. Personally, spending time with a good book brings a unique kind of joy and peace.\"\"\"\n",
        "text = paragraph.lower()\n",
        "text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "sent_tokens = sent_tokenize(paragraph)\n",
        "word_tokens = word_tokenize(text)\n",
        "\n",
        "split_words = text.split()\n",
        "print(\"Using split():\", split_words)\n",
        "print(\"\\nUsing word_tokenize():\", word_tokens)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in word_tokens if word not in stop_words]\n",
        "\n",
        "word_freq = Counter(filtered_words)\n",
        "print(\"\\nWord Frequency (excluding stopwords):\")\n",
        "for word, freq in word_freq.items():\n",
        "    print(f\"{word}: {freq}\")\n"
      ],
      "metadata": {
        "id": "N5LQWw0IMz6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Using the same paragraph from Q1:\n",
        "1. Extract all words with only alphabets using re.findall()\n",
        "2. Remove stop words using NLTK’s stopword list\n",
        "3. Perform stemming with PorterStemmer\n",
        "4. Perform lemmaƟzaƟon with WordNetLemmaƟzer\n",
        "5. Compare the stemmed and lemmaƟzed outputs and explain when you’d prefer one over\n",
        "the other."
      ],
      "metadata": {
        "id": "LOQfKdnVNFyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "paragraph = \"\"\"Books open the door to countless worlds, ideas, and perspectives. Reading provides an escape from reality and offers a chance to learn something new every day. Whether it’s fiction or nonfiction, books allow deep connections with characters and concepts. They stimulate the imagination and enhance vocabulary. Personally, spending time with a good book brings a unique kind of joy and peace.\"\"\"\n",
        "text = paragraph.lower()\n",
        "\n",
        "alpha_words = re.findall(r'\\b[a-z]+\\b', text)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in alpha_words if word not in stop_words]\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "print(\"Stemmed Words:\", stemmed_words)\n",
        "print(\"\\nLemmatized Words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "id": "hthRs135NRXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Choose 3 short texts of your own (e.g., different news headlines, product reviews).\n",
        "1. Use CountVectorizer to generate the Bag of Words representaƟon.\n",
        "2. Use TfidfVectorizer to compute TF-IDF scores.\n",
        "3. Print and interpret the top 3 keywords from each text using TF-IDF."
      ],
      "metadata": {
        "id": "VxxaMNazNdQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "texts = [\n",
        "    \"The battery life of this phone is amazing and lasts all day.\",\n",
        "    \"Customer service was terrible and the response was very slow.\",\n",
        "    \"The camera quality is outstanding and takes crystal clear photos.\"\n",
        "]\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "count_matrix = count_vectorizer.fit_transform(texts)\n",
        "print(\"Bag of Words Representation:\\n\")\n",
        "print(count_matrix.toarray())\n",
        "print(\"\\nFeature Names:\", count_vectorizer.get_feature_names_out())\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"\\nTF-IDF Scores:\\n\")\n",
        "\n",
        "for i, text in enumerate(texts):\n",
        "    print(f\"\\nText {i+1}: {text}\")\n",
        "    tfidf_scores = tfidf_matrix[i].toarray().flatten()\n",
        "    top_indices = np.argsort(tfidf_scores)[-3:][::-1]\n",
        "    top_keywords = [(feature_names[idx], round(tfidf_scores[idx], 3)) for idx in top_indices]\n",
        "    print(\"Top 3 Keywords (TF-IDF):\", top_keywords)\n"
      ],
      "metadata": {
        "id": "4KEisiDBODcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Write 2 short texts (4–6 lines each) describing two different technologies (e.g., AI vs\n",
        "Blockchain).\n",
        "1. Preprocess and tokenize both texts.\n",
        "2. Calculate:\n",
        "a. Jaccard Similarity using sets\n",
        "b. Cosine Similarity using TfidfVectorizer + cosine_similarity()\n",
        "\n",
        "c. Analyze which similarity metric gives beƩer insights in your case."
      ],
      "metadata": {
        "id": "BIGB3b6POC4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text1 = \"\"\"Artificial Intelligence enables machines to learn from data and make decisions.\n",
        "It is widely used in applications like speech recognition, recommendation systems, and autonomous vehicles.\"\"\"\n",
        "\n",
        "text2 = \"\"\"Blockchain is a decentralized ledger that records transactions across multiple computers.\n",
        "It is mostly used in cryptocurrency systems and enhances security and transparency.\"\"\"\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered = [word for word in tokens if word not in stop_words]\n",
        "    return filtered\n",
        "\n",
        "tokens1 = preprocess(text1)\n",
        "tokens2 = preprocess(text2)\n",
        "\n",
        "set1 = set(tokens1)\n",
        "set2 = set(tokens2)\n",
        "intersection = set1.intersection(set2)\n",
        "union = set1.union(set2)\n",
        "jaccard_sim = len(intersection) / len(union)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
        "cos_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "\n",
        "print(\"Jaccard Similarity:\", round(jaccard_sim, 3))\n",
        "print(\"Cosine Similarity:\", round(cos_sim, 3))\n"
      ],
      "metadata": {
        "id": "SzQ9T6zzN2ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Write a short review for a product or service.\n",
        "1. Use TextBlob or VADER to find polarity & subjecƟvity for each review.\n",
        "2. Classify reviews into PosiƟve / NegaƟve / Neutral.\n",
        "3. Create a word cloud using the wordcloud library for all posiƟve reviews."
      ],
      "metadata": {
        "id": "SLwWr770N_Xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "reviews = [\n",
        "    \"This phone has excellent battery life and a sleek design.\",\n",
        "    \"The delivery was delayed and the customer support was unhelpful.\",\n",
        "    \"It’s an average product. Nothing too great or too bad.\",\n",
        "    \"I absolutely love the camera quality and performance!\",\n",
        "    \"The sound quality is poor and it stopped working in a week.\"\n",
        "]\n",
        "\n",
        "def classify_sentiment(polarity):\n",
        "    if polarity > 0.1:\n",
        "        return \"Positive\"\n",
        "    elif polarity < -0.1:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "review_data = []\n",
        "\n",
        "for review in reviews:\n",
        "    blob = TextBlob(review)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    subjectivity = blob.sentiment.subjectivity\n",
        "    sentiment = classify_sentiment(polarity)\n",
        "    review_data.append((review, polarity, subjectivity, sentiment))\n",
        "\n",
        "for r in review_data:\n",
        "    print(f\"\\nReview: {r[0]}\")\n",
        "    print(f\"Polarity: {round(r[1], 3)} | Subjectivity: {round(r[2], 3)} | Sentiment: {r[3]}\")\n",
        "\n",
        "positive_reviews = \" \".join([r[0] for r in review_data if r[3] == \"Positive\"])\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(positive_reviews)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud for Positive Reviews\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HB6g1VuzOnZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Choose your own paragraph (~100 words) as training data.\n",
        "1. Tokenize text using Tokenizer() from keras.preprocessing.text\n",
        "2. Create input sequences and build a simple LSTM or Dense model\n",
        "3. Train the model and generate 2–3 new lines of text starƟng from any seed word you\n",
        "provide."
      ],
      "metadata": {
        "id": "9VJaDp1NOmU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "import tensorflow as tf\n",
        "\n",
        "text = \"\"\"\n",
        "Technology continues to revolutionize the way people interact, work, and live.\n",
        "From artificial intelligence to renewable energy, every innovation shapes the world around us.\n",
        "Smart devices make daily tasks more efficient, while data-driven decisions lead to smarter solutions.\n",
        "Cloud computing has redefined accessibility and collaboration across industries.\n",
        "As we advance, ethical considerations must guide how technology is used.\n",
        "The future holds endless possibilities when innovation is paired with responsibility.\n",
        "\"\"\"\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "input_sequences = []\n",
        "token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "for i in range(2, len(token_list)):\n",
        "    n_gram_seq = token_list[:i]\n",
        "    input_sequences.append(n_gram_seq)\n",
        "\n",
        "max_seq_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre'))\n",
        "\n",
        "X = input_sequences[:, :-1]\n",
        "y = tf.keras.utils.to_categorical(input_sequences[:, -1], num_classes=total_words)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 10, input_length=max_seq_len-1))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X, y, epochs=300, verbose=0)\n",
        "\n",
        "def generate_text(seed_text, next_words=15):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
        "        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)[0]\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                seed_text += ' ' + word\n",
        "                break\n",
        "    return seed_text\n",
        "\n",
        "seed = \"technology\"\n",
        "generated = generate_text(seed, next_words=15)\n",
        "print(\"\\nGenerated Text:\\n\", generated)\n"
      ],
      "metadata": {
        "id": "cAZXbUNLO-gN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}